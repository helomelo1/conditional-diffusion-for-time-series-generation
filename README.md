# Conditional Diffusion for Time-Series Generation

This project implements a **denoising diffusion probabilistic model (DDPM)** from scratch to generate realistic **time-series sequences**, using financial log-returns as a concrete example.

The objective is **generative modeling**, not forecasting.

---

## Overview

Diffusion models are typically applied to images, but many real-world problems involve **1D sequential data** such as sensor signals, biomedical waveforms, or financial returns.

This project investigates whether diffusion models can:
- Learn the distribution of time-series sequences
- Preserve temporal structure
- Generate realistic samples from noise

---

## Data

- Daily closing prices of the S&P 500 index
- Prices are converted to **log-returns** for stationarity
- Fixed-length sliding windows of 128 timesteps
- Global normalization across the dataset

Each sequence is treated as a single data object.

> This is **not** a time-series forecasting task.

---

## Method

### Diffusion Framework
- Forward process: Gaussian noise is gradually added to real sequences
- Reverse process: a neural network learns to iteratively remove noise
- Training objective: predict the added noise using mean squared error (MSE)

### Model Architecture
- 1D UNet with temporal convolutions
- Skip connections for multi-scale feature preservation
- Linear output layer (no activation) to predict noise

---

## Training Objective

The diffusion model is trained to predict the Gaussian noise added during the forward diffusion process.

At each training step:
- A clean sequence x0 is corrupted with noise to obtain xt
- The model learns to predict the exact noise used in this corruption

Formally, the objective is to minimize the mean squared error between:
- the true noise ε
- the model prediction ε̂ = fθ(xt, t)

Loss = mean( (ε − ε̂)² )

This corresponds to maximum likelihood training under the diffusion framework and is standard in DDPM-based models.

---

## Sampling

New sequences are generated by:
1. Sampling pure Gaussian noise
2. Iteratively applying the learned reverse diffusion process
3. Producing a synthetic time-series sequence

The generated outputs are stochastic, temporally coherent, and statistically similar to real data.

---

## Evaluation

Since this is a generative model, evaluation is **distributional**, not predictive.

Evaluation focuses on:
- Mean and standard deviation
- Visual comparison of real vs generated sequences
- Qualitative temporal structure

Further analysis may include autocorrelation and volatility structure.

---

## Running the Code

### 1. Download data
Download and save daily closing prices (CSV format) in the working directory or a `data/` folder.

Example:
```bash
python download_data.py